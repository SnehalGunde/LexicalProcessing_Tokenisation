{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e70719",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "- The notebook contains three types of tokenisation techniques:\n",
    "\n",
    "1. Word tokenisation\n",
    "2. Sentence tokenisation\n",
    "3. Tweet tokenisation\n",
    "4. Custom tokenisation using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebe30f",
   "metadata": {},
   "source": [
    "### 1. Word Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312677e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\n"
     ]
    }
   ],
   "source": [
    "document = \"At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\"\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd356ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'nine', \"o'clock\", 'I', 'visited', 'him', 'myself.', 'It', 'looks', 'like', 'religious', 'mania,', 'and', \"he'll\", 'soon', 'think', 'that', 'he', 'himself', 'is', 'God.']\n"
     ]
    }
   ],
   "source": [
    "print(document.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f1254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80b03d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function word_tokenize in module nltk.tokenize:\n",
      "\n",
      "word_tokenize(text, language='english', preserve_line=False)\n",
      "    Return a tokenized copy of *text*,\n",
      "    using NLTK's recommended word tokenizer\n",
      "    (currently an improved :class:`.TreebankWordTokenizer`\n",
      "    along with :class:`.PunktSentenceTokenizer`\n",
      "    for the specified language).\n",
      "    \n",
      "    :param text: text to split into words\n",
      "    :type text: str\n",
      "    :param language: the model name in the Punkt corpus\n",
      "    :type language: str\n",
      "    :param preserve_line: A flag to decide whether to sentence tokenize the text or not.\n",
      "    :type preserve_line: bool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9b3b5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'nine',\n",
       " \"o'clock\",\n",
       " 'I',\n",
       " 'visited',\n",
       " 'him',\n",
       " 'myself',\n",
       " '.',\n",
       " 'It',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'religious',\n",
       " 'mania',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " \"'ll\",\n",
       " 'soon',\n",
       " 'think',\n",
       " 'that',\n",
       " 'he',\n",
       " 'himself',\n",
       " 'is',\n",
       " 'God',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words= word_tokenize(document)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4727f5",
   "metadata": {},
   "source": [
    "- NLTK's word tokeniser not only breaks on whitespaces but also breaks contraction words such as he'll into \"he\" and \"'ll\". On the other hand it doesn't break \"o'clock\" and treats it as a separate token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26580a60",
   "metadata": {},
   "source": [
    "### 2. Sentence tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240eca02",
   "metadata": {},
   "source": [
    "Tokenising based on sentence requires you to split on the period ('.'). Let's use nltk sentence tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f50c4d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21583377",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence= sent_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74bc20e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At nine o'clock I visited him myself.\", \"It looks like religious mania, and he'll soon think that he himself is God.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d5c2cd",
   "metadata": {},
   "source": [
    "### 3. Tweet tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140220ed",
   "metadata": {},
   "source": [
    "A problem with word tokeniser is that it fails to tokeniser emojis and other complex special characters such as word with hashtags. Emojis are common these days and people use them all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cc0195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"i recently watched this show called mindhunters:). i totally loved it üòç. it was gr8 <3. #bingewatching #nothingtodo üòé\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef489d8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'recently', 'watched', 'this', 'show', 'called', 'mindhunters', ':', ')', '.', 'i', 'totally', 'loved', 'it', 'üòç', '.', 'it', 'was', 'gr8', '<', '3', '.', '#', 'bingewatching', '#', 'nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5023c62",
   "metadata": {},
   "source": [
    "The word tokeniser breaks the emoji '<3' into '<' and '3' which is something that we don't want. Emojis have their own significance in areas like sentiment analysis where a happy face and sad face can salone prove to be a really good predictor of the sentiment. Similarly, the hashtags are broken into two tokens. A hashtag is used for searching specific topics or photos in social media apps such as Instagram and facebook. So there, you want to use the hashtag as is.\n",
    "\n",
    "Let's use the tweet tokeniser of nltk to tokenise this message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef59b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61ba834e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'recently',\n",
       " 'watched',\n",
       " 'this',\n",
       " 'show',\n",
       " 'called',\n",
       " 'mindhunters',\n",
       " ':)',\n",
       " '.',\n",
       " 'i',\n",
       " 'totally',\n",
       " 'loved',\n",
       " 'it',\n",
       " 'üòç',\n",
       " '.',\n",
       " 'it',\n",
       " 'was',\n",
       " 'gr8',\n",
       " '<3',\n",
       " '.',\n",
       " '#bingewatching',\n",
       " '#nothingtodo',\n",
       " 'üòé']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr.tokenize(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8510b3b5",
   "metadata": {},
   "source": [
    "### 4. Custom tokenisation using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c2f519",
   "metadata": {},
   "source": [
    "As you can see, it handles all the emojis and the hashtags pretty well.\n",
    "\n",
    "Now, there is a tokeniser that takes a regular expression and tokenises and returns result based on the pattern of regular expression.\n",
    "\n",
    "Let's look at how you can use regular expression tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cb3c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "message = \"i recently watched this show called mindhunters:). i totally loved it üòç. it was gr8 <3. #bingewatching #nothingtodo üòé\"\n",
    "pattern = \"#[\\w]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "182a38fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#bingewatching', '#nothingtodo']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(message, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c03527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
